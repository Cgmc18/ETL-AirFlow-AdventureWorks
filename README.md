# Projeto de Engenharia de Dados: Data Warehouse AdventureWorks

Este projeto consiste na construÃ§Ã£o de um **Data Warehouse (DW)** completo, desde a modelagem multidimensional atÃ© a orquestraÃ§Ã£o de pipelines ETL utilizando **Apache Airflow**, **Docker** e **PostgreSQL**.

O objetivo foi transformar dados transacionais (OLTP) do banco de dados fictÃ­cio **AdventureWorks** em um modelo analÃ­tico (OLAP) capaz de responder a indicadores de negÃ³cio.

---

## ðŸ› ï¸ Tecnologias Utilizadas

* **Linguagem:** Python 3.x (Pandas, SQLAlchemy)
* **OrquestraÃ§Ã£o:** Apache Airflow 3.x (via Docker)
* **Banco de Dados (DW):** PostgreSQL 16
* **ContainerizaÃ§Ã£o:** Docker & Docker Compose
* **IDE/Ferramentas:** VS Code, DBeaver

---

## ðŸ—ï¸ Arquitetura e Modelagem

Foi adotado o esquema **Star Schema (Modelo Estrela)**, ideal para consultas analÃ­ticas rÃ¡pidas.

### O Modelo
* **Fato:** `Fato_Vendas` (Granularidade: Item do pedido)
* **DimensÃµes:**
    * `Dim_Produto` (Dados de produtos, categorias e subcategorias)
    * `Dim_Cliente` (Dados unificados de Pessoa e Cliente)
    * `Dim_Vendedor` (Dados de funcionÃ¡rios e vendas)
    * `Dim_Localidade` (PaÃ­s, Estado e Cidade)
    * `Dim_Tempo` (Gerada via cÃ³digo para anÃ¡lises temporais)

> ![Diagrama](diagrama.png)

---

## ðŸ”„ Pipeline ETL (Extract, Transform, Load)

Os processos de ETL foram desenvolvidos utilizando DAGs (Directed Acyclic Graphs) no Airflow, utilizando a biblioteca **Pandas** para transformaÃ§Ã£o de dados em memÃ³ria.

### Fluxo das DAGs:
1.  **Extract:** Leitura de arquivos CSV brutos (`SalesOrder`, `Customer`, `Product`, etc.) e leitura de tabelas do prÃ³prio banco (para Lookups).
2.  **Transform:**
    * Limpeza de dados nulos.
    * RenomeaÃ§Ã£o de colunas conflitantes (ex: `Name` de PaÃ­s vs `Name` de Estado).
    * Cruzamento de dados (Joins) para desnormalizaÃ§Ã£o das dimensÃµes.
    * GeraÃ§Ã£o de chaves substitutas (Surrogate Keys).
    * CÃ¡lculo de mÃ©tricas (Valor Total, Descontos).
3.  **Load:** Carregamento incremental (`append`) ou total (`truncate/insert`) no PostgreSQL.

### Lista de DAGs:
* `etl_dim_produto`: Consolida Produto, Categoria e Subcategoria.
* `etl_dim_cliente`: Unifica tabelas Customer e Person.
* `etl_dim_localidade`: Hierarquia de EndereÃ§o, Estado e PaÃ­s.
* `etl_dim_vendedor`: Cruzamento de FuncionÃ¡rio e Vendedor.
* `etl_dim_tempo`: GeraÃ§Ã£o automÃ¡tica de calendÃ¡rio (2010-2025).
* **`etl_fato_vendas`**: A DAG principal que cruza os pedidos com todas as dimensÃµes acima.

---

## ðŸ“Š Indicadores de NegÃ³cio (KPIs)

O Data Warehouse responde a 10 perguntas chaves de negÃ³cio, validadas via SQL:

1.  Faturamento Bruto Total
2.  Faturamento LÃ­quido Total
3.  Ticket MÃ©dio
4.  Volume Total de Vendas (Quantidade)
5.  Top 5 Produtos Mais Vendidos
6.  Performance de Vendas por PaÃ­s
7.  Sazonalidade Mensal de Vendas
8.  Vendas por Categoria de Produto
9.  Total de Descontos Concedidos
10. Ranking de Melhores Vendedores

---

## ðŸš§ Desafios e SoluÃ§Ãµes

Durante o desenvolvimento, diversos desafios tÃ©cnicos foram superados:

### 1. Mapeamento de Volumes no Docker (Windows)
* **Problema:** O Airflow rodando no Docker nÃ£o conseguia enxergar os arquivos `.py` e `.csv` locais devido a caminhos absolutos incorretos no Windows.
* **SoluÃ§Ã£o:** Ajuste no `docker-compose.yaml` para usar caminhos relativos (`./dags:/opt/airflow/dags`), garantindo o espelhamento correto das pastas.

### 2. DependÃªncias Python (Pandas)
* **Problema:** A imagem oficial do Airflow nÃ£o inclui as bibliotecas `pandas` e `sqlalchemy` por padrÃ£o, gerando erros de importaÃ§Ã£o ("ModuleNotFoundError").
* **SoluÃ§Ã£o:** ConfiguraÃ§Ã£o da variÃ¡vel de ambiente `_PIP_ADDITIONAL_REQUIREMENTS` no Docker Compose para instalar as dependÃªncias automaticamente na inicializaÃ§Ã£o dos containers.

### 3. Conectividade e PermissÃµes
* **Problema:** O container do Airflow perdia a conexÃ£o com o banco apÃ³s reinicializaÃ§Ãµes ("Connection ID not found").
* **SoluÃ§Ã£o:** CriaÃ§Ã£o persistente da conexÃ£o `postgres_dw` via interface do Airflow e configuraÃ§Ã£o correta do host interno do Docker (`postgres` ou `host.docker.internal`).

---

## ðŸš€ Como executar este projeto

### PrÃ©-requisitos
* Docker e Docker Compose instalados.
* Arquivos CSV do AdventureWorks na pasta `dags/data`.

### Passo a Passo
1.  Clone o repositÃ³rio:
    ```bash
    git clone https://github.com/Cgmc18/ETL-AirFlow-AdventureWorks.git
    ```
2.  Suba o ambiente Docker:
    ```bash
    docker-compose up -d
    ```
3.  Crie as tabelas no Banco de Dados (Postgres) executando o script SQL localizado em `/sql/create_dw_tables.sql`.
4.  Acesse o Airflow em `http://localhost:8080`.
5.  Configure a conexÃ£o `postgres_dw` no Admin do Airflow.
6.  Ative e execute as DAGs na ordem: DimensÃµes -> Fato.

---

### Autor
Ana Carolina Gomes.